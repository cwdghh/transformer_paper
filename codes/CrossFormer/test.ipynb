{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.crossformer import Attention\n",
    "from models.crossformer import DynamicPosBias\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "9\n",
      "7\n",
      "9\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "14\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "depths = [2, 2, 18, 2]\n",
    "layers = len(depths)\n",
    "patches_resolution = [56, 56]\n",
    "min_size = 4\n",
    "total_depth = sum(depths)\n",
    "step_size = (1 - min_size / patches_resolution[0]) / total_depth\n",
    "group_fraction = np.arange(min_size / patches_resolution[0], 1.0, step_size)\n",
    "# print(len(group_fraction))\n",
    "\n",
    "cnt = 0\n",
    "for i_layer in range(layers):\n",
    "    cur_resolution = patches_resolution[0] // 2 ** i_layer\n",
    "    for j in range(depths[i_layer]):\n",
    "        group_size = cur_resolution * group_fraction[cnt]\n",
    "        if group_size > cur_resolution // 2:\n",
    "            group_size = cur_resolution if group_size > cur_resolution * 3 / 4 else cur_resolution // 2\n",
    "        print(max(4, int(np.ceil(group_size))))\n",
    "        cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0.], device='cuda:0')\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([[    -0., -10000., -10000., -10000.],\n",
      "        [-10000., -10000., -10000., -10000.],\n",
      "        [-10000., -10000., -10000., -10000.],\n",
      "        [-10000., -10000., -10000., -10000.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((7, 7)).cuda()\n",
    "a = F.pad(a, [2, 1, 2, 1])\n",
    "a = a.reshape((5, 2, 5, 2)).permute((0, 2, 1, 3)).flatten(-2).flatten(0, 1)\n",
    "print(a[24])\n",
    "a = a.unsqueeze(-1) * a.unsqueeze(-2)\n",
    "print(a[24])\n",
    "a = (1 - a) * -10000\n",
    "print(a[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([144, 25, 128])\n",
      "torch.Size([16, 14, 14, 128])\n"
     ]
    }
   ],
   "source": [
    "C = 128\n",
    "n_head = 8\n",
    "B = 16\n",
    "G = 5\n",
    "H, W = 14, 14\n",
    "\n",
    "attn = Attention(C, (G, G), n_head).cuda()\n",
    "\n",
    "mask = torch.ones((H, W)).cuda()\n",
    "pad_num = (int(np.ceil(H / G))) * G - H\n",
    "if pad_num % 2 == 0:\n",
    "    pad_size = [pad_num // 2] * 4\n",
    "else:\n",
    "    pad_size = [(pad_num - 1) // 2, (pad_num + 1) // 2] * 2\n",
    "\n",
    "mask = F.pad(mask, pad_size)\n",
    "mask = mask.reshape((int(np.ceil(H / G)), G, int(np.ceil(H / G)), G)).permute((0, 2, 1, 3)).flatten(-2).flatten(0, 1)\n",
    "mask = mask.unsqueeze(-1) * mask.unsqueeze(-2)\n",
    "mask = (1 - mask) * -10000\n",
    "\n",
    "x = torch.rand((B, H * W, C)).cuda()\n",
    "x = x.view(B, H, W, C)\n",
    "x_pad = F.pad(x, [0, 0] + pad_size)\n",
    "x_pad = x_pad.reshape(B, int(np.ceil(H / G)), G, int(np.ceil(H / G)), G, C).permute(0, 1, 3, 2, 4, 5)\n",
    "x_pad = x_pad.reshape(-1, G**2, C)\n",
    "\n",
    "print(x_pad.shape)\n",
    "\n",
    "out_pad = attn(x_pad, mask)\n",
    "out_pad = out_pad.reshape(B, int(np.ceil(H / G)), int(np.ceil(H / G)), G, G, C)\n",
    "out_pad = out_pad.permute(0, 1, 3, 2, 4, 5).reshape(B, H + pad_num, W + pad_num, C)\n",
    "out = out_pad[:, pad_num // 2: pad_num // 2 + H, pad_num // 2: pad_num // 2 + W, :]\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1455, 0.2386, 0.0221, 0.0086, 0.0000],\n",
      "        [0.5513, 0.9299, 0.7500, 0.5527, 0.0000],\n",
      "        [0.6135, 0.8574, 0.8102, 0.8803, 0.0000],\n",
      "        [0.9937, 0.5681, 0.9847, 0.0302, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0')\n",
      "tensor([[-0., -0., -0., -0., -1.],\n",
      "        [-0., -0., -0., -0., -1.],\n",
      "        [-0., -0., -0., -0., -1.],\n",
      "        [-0., -0., -0., -0., -1.],\n",
      "        [-1., -1., -1., -1., -1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(x_pad[-1, :, 0].reshape(G, G))\n",
    "print(mask[-1, 0].reshape(G, G) / 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mutual_Attention(nn.Module):\n",
    "    r\"\"\" Multi-head self attention module with dynamic position bias.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        group_size (tuple[int]): The height and width of the group.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, group_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.,\n",
    "                 position_bias=True):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.group_size = group_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.position_bias = position_bias\n",
    "\n",
    "        self.q  = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, y, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_groups*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_groups, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        _,  L, C = y.shape\n",
    "        q  = self.q(x).reshape(B_, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        kv = self.kv(y).reshape(B_, L, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv[0], kv[1]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        # @ stands for matrix multiplication\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, group_size={self.group_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 group with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        if self.position_bias:\n",
    "            flops += self.pos.flops(N)\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 128\n",
    "n_head = 8\n",
    "B = 16\n",
    "G = 7\n",
    "H, W = 14, 14\n",
    "\n",
    "attn = Mutual_Attention(C, (G, G), n_head, position_bias=False).cuda()\n",
    "depth_conv = nn.Conv2d(in_channels=C, out_channels=C, kernel_size=3, padding=1, groups=C).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 14, 14, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((B, H, W, C)).cuda()\n",
    "x = x.permute(0, 3, 1, 2)\n",
    "x = x + depth_conv(x)\n",
    "x = x.permute(0, 2, 3, 1)\n",
    "y = x.reshape(B, H // G, G, W // G, G, C).permute(0, 1, 3, 2, 4, 5)\n",
    "y = y.reshape(B, H * W // G**2, G**2, C).mean(dim=2)\n",
    "x = x.reshape(B, H * W, C)\n",
    "\n",
    "out = attn(x, y)\n",
    "out = out.reshape(B, H, W, C)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pad_Attention(nn.Module):\n",
    "    r\"\"\" Multi-head self attention module with dynamic position bias.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        group_size (tuple[int]): The height and width of the group.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, group_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.,\n",
    "                 position_bias=True):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.group_size = group_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.position_bias = position_bias\n",
    "\n",
    "        if position_bias:\n",
    "            self.pos = DynamicPosBias(self.dim // 4, self.num_heads, residual=False)\n",
    "            \n",
    "            # generate mother-set\n",
    "            position_bias_h = torch.arange(1 - self.group_size[0], self.group_size[0])\n",
    "            position_bias_w = torch.arange(1 - self.group_size[1], self.group_size[1])\n",
    "            biases = torch.stack(torch.meshgrid([position_bias_h, position_bias_w]))  # 2, 2Wh-1, 2Ww-1\n",
    "            biases = biases.flatten(1).transpose(0, 1).float()\n",
    "            self.register_buffer(\"biases\", biases)\n",
    "\n",
    "            # get pair-wise relative position index for each token inside the group\n",
    "            coords_h = torch.arange(self.group_size[0])\n",
    "            coords_w = torch.arange(self.group_size[1])\n",
    "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "            relative_coords[:, :, 0] += self.group_size[0] - 1  # shift to start from 0\n",
    "            relative_coords[:, :, 1] += self.group_size[1] - 1\n",
    "            relative_coords[:, :, 0] *= 2 * self.group_size[1] - 1\n",
    "            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (4, num_groups*B, N, C // 4)\n",
    "            mask: (0/-inf) mask with shape of (4, num_groups, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        _, B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(4, B_, N, 3, self.num_heads, C // self.num_heads).permute(3, 0, 1, 4, 2, 5)\n",
    "        # q, k, v shape: [4, B, nH, N, C // nH]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        # @ stands for matrix multiplication\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        if self.position_bias:\n",
    "            pos = self.pos(self.biases) # 2Wh-1 * 2Ww-1, heads\n",
    "            # select position bias\n",
    "            relative_position_bias = pos[self.relative_position_index.view(-1)].view(\n",
    "                self.group_size[0] * self.group_size[1], self.group_size[0] * self.group_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "            attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[1]\n",
    "            attn = attn.view(4, B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(2).unsqueeze(1)\n",
    "            attn = attn.view(4, -1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(2, 3).reshape(4, B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, group_size={self.group_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 group with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        if self.position_bias:\n",
    "            flops += self.pos.flops(N)\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 144, 144])\n"
     ]
    }
   ],
   "source": [
    "C = 128\n",
    "n_head = 8\n",
    "B = 16\n",
    "G = 12\n",
    "H, W = 14, 14\n",
    "\n",
    "attn = Pad_Attention(C // 4, (G, G), n_head).cuda()\n",
    "\n",
    "mask = torch.ones((H, W)).cuda()\n",
    "pad_num  = (int(np.ceil(H / G))) * G - H\n",
    "pad_size = [pad_num, 0, pad_num, 0]\n",
    "\n",
    "mask = F.pad(mask, pad_size)\n",
    "mask = torch.stack([mask, torch.fliplr(mask), torch.flipud(mask), torch.fliplr(torch.flipud(mask))], dim=0)\n",
    "mask = mask.reshape(4, int(np.ceil(H / G)), G, int(np.ceil(H / G)), G).permute(0, 1, 3, 2, 4).flatten(-2).flatten(1, 2)\n",
    "mask = mask.unsqueeze(-1) * mask.unsqueeze(-2)\n",
    "mask = (1 - mask) * -10000\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 144, 32])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((B, H * W, C)).cuda()\n",
    "x = x.view(B, H, W, C)\n",
    "\n",
    "new_C = C // 4\n",
    "x_pad = torch.stack([\n",
    "    F.pad(x[:, :, :, :C//4],       [0, 0, pad_num, 0, pad_num, 0]), \n",
    "    F.pad(x[:, :, :, C//4:C//2],   [0, 0, 0, pad_num, pad_num, 0]), \n",
    "    F.pad(x[:, :, :, C//2:3*C//4], [0, 0, pad_num, 0, 0, pad_num]), \n",
    "    F.pad(x[:, :, :, 3*C//4:],     [0, 0, 0, pad_num, 0, pad_num])], \n",
    "    dim=0) # [4, B, H + pad_num, W + pad_num, C // 4]\n",
    "x_pad = x_pad.reshape(4, B, (H + pad_num) // G, G, (W + pad_num) // G, G, new_C).permute(0, 1, 2, 4, 3, 5, 6)\n",
    "x_pad = x_pad.reshape(4, -1, G**2, new_C)\n",
    "\n",
    "print(x_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 144, 32])\n",
      "torch.Size([16, 14, 14, 128])\n"
     ]
    }
   ],
   "source": [
    "out = attn(x_pad, mask)\n",
    "print(out.shape)\n",
    "out = out.reshape(4, B, (H + pad_num) // G, (W + pad_num) // G, G, G, new_C)\n",
    "out = out.permute(0, 1, 2, 4, 3, 5, 6).reshape(4, B, H + pad_num, W + pad_num, new_C)\n",
    "out = torch.cat([\n",
    "    out[0, :, pad_num:, pad_num:, :], \n",
    "    out[1, :, pad_num:, :W, :],\n",
    "    out[2, :, :H, pad_num:, :],\n",
    "    out[3, :, :H, :W, :]], \n",
    "    dim=-1)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('crossformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7088eb542b1225d97be3d56cfa8fc251e3c4a835b0b11e99e1f92350f5c0ac35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
